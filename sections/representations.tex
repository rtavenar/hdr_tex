Another track of research I have been following over the past years is the
learning of latent representations for time series.
These latent representations can either be mixture coefficients
(\emph{cf.} Sec.~\ref{sec:topics}) -- in which case time series are
represented as multinomial distributions over latent topics -- or intermediate
neural networks feature maps (as in Sec.~\ref{sec:cnn} and
Sec.~\ref{sec:early}) -- and then time series are represented through
filter activations they trigger.

More specifically, in Sec.~\ref{sec:early}, we focus on the task of early
classification of time series. In this context, a method is introduced.
This method
learns an intermediate representation from which both the decision of
triggering classification and the classification itself can be computed.


\input{sections/02/topic_models.tex}
\input{sections/02/shapelets_cnn.tex}
\input{sections/02/early.tex}
